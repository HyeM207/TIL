# 웹 크롤링과 웹 스크랩핑 
## 📌웹 크롤링과 웹 스크랩핑 차이
- `웹 크롤링` : 특정 목적을로 __특정 웹 페이지__ 에서 데이터를 추출하는 것 (데이터 추출) 
    - ex. 날씨 데이터 혹은 주식 데이터 가져오기
- `웹 스크랩핑` : URL을 타고다니며 반복적으로 데이터를 가져오는 과정 (데이터 색인/인덱싱)
    - ex. 검색 엔진의 웹 크롤러

<br>

## 📌 주의할 점 
1. 웹 스크래핑/크롤링한 정보를 __상업적 목적__ 에 쓰면 안됨
2. DDoS 공격처럼 과도한 요청을 하여 __서버에 영향을 주면 안 됨__ 
3. REP(Robot Exclusion Protocol)의 __robot.txt에 맞춰서__ 진행

<br>

## 📌 Robots.txt
> robots.txt는 웹 사이트 및 웹 페이지를 수집하는 로봇들의 무단 접근을 방지하기 위해 만들어진 로봇 백제 표준이자 국제 권고안이다.


사이트 __기본 도메인 주소에 /robots.txt를 붙여__ 요청하면 나오는 파일로, 크롤링을 허가하는 범위와 대상을 확인할 수 있다. 

웹 크롤러들은 해당 규칙을 지키면서 크롤링을 진행해야 한다.   

웹 서버에 요청시 `user agent HTTP header`에 나의 브라우저 정보를 전달하면 __웹 서버가 나를 진짜 사용자__ 로 인식할 수 있다. 이때  `user-agent`(사용자 에이전트)란 요청하는 브라우저에 전달하는 것으로 요청자의 정보 파악할 수 있다. 
- https://www.whatismybrowser.com/detect/what-is-my-user-agent/ 에서 확인 가능

<br>

### ✅ 요청 예시
- `User-agent` : 규칙이 적용되는 대상 사용자 에이전트
- `Disallow` : 크롤링 금지할 웹 사이트 
- `Allow` : 크롤링을 허용할 웹 페이지 

[예시 1]
```txt
User-agent: *
Disallow : /
```
- 모든 user-agent의 접근 거부

[예시 1]
```txt
User-agent: *
Allow : /
Disallow : /user
```
- 모든 user-agent에 대해 /user를 제외한 모든 접근 허가 

<br>

### ✅ Python으로 실습하기
```python
import requests
res = requests.get("https://www.naver.com/robots.txt")

print(res.text)
```
User-agent: *  
Disallow: /  
Allow : /$   
 

```python
import requests
res = requests.get("https://comic.naver.com/robots.txt")

print(res.text)
```
User-agent: *  
Disallow: /  

User-agent: Yeti  
Disallow: /  
Allow: /navercontest/2021/  

<br><br>


## 📌 DOM (Document Object Model)
브라우저의 렌더링 엔진은 웹 문서를 로드한 후 __파싱__ 을 진행하여 __DOM__ 을 생성함
- 파싱을 진행하면 html을 head와 body등으로 나누게 됨 
- DOM Tree를 순회해서 __원하는 특정 원소__ 를 찾거나 변경(추가)할 수 있다. 


파이썬으로 HTML을 분석하는 HTML Parser가 필요함 => BeautifulSoup